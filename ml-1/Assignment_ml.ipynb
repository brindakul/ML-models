{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 613 - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Classification\n",
    "\n",
    "Name <br>\n",
    "CS 613 Machine Learning <br>\n",
    "Fall 2021 <br>\n",
    "Dr. Edward Kim <br>\n",
    "Drexel University <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment you will perform classification using Logistic Regression, Naive Bayes and Decision Tree classifiers.  You will run your implementations on  a binary class dataset and report your results.\n",
    "\n",
    "You may __NOT__ use any functions from a ML library in your code unless explicitly told otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "For your submission, upload to Blackboard a single zip file containing:\n",
    "1.  PDF Writeup and PDF of Jupyter Notebook (can be the same PDF)\n",
    "2.  Python notebook Code\n",
    "\n",
    "The PDF document should contain the following at the top:\n",
    "\n",
    "__1. Answers to Theory Questions__\n",
    "\n",
    "_1.1_\n",
    "- a. What is the sample entropy, ùêª(ùëå) from this training data (using log base 2) (2pts)?\n",
    "- b. What are the information gains for branching on variables $x_1$ and $x_2$ (2pts)? <br>\n",
    "- c. Draw the decision tree that would be learned by the ID3 algorithm without pruning from this training data (3pts)?\n",
    "\n",
    "\n",
    "_1.2_\n",
    "- a. What are the class priors, $P(A=Yes), P(A=No)$? (2pt)\n",
    "- b. Find the parameters of the Gaussians necessary to do Gaussian Naive Bayes classification on this decision to give an A or not.  Standardize the features first over all the data together so that there is no unfair bias towards the features of different scales (2pts).\n",
    "- c. Using your response from the prior question, determine if an essay with 242 characters and an average word length of 4.56 should get an A or not (3pts).\n",
    "\n",
    "\n",
    "_1.3_\n",
    "- a. How could you use a validation set to determine the user-defined parameter $k$?\n",
    "\n",
    "__2. Requested Logistic Regression thetas and plots__\n",
    "\n",
    "__3. Requested Classification Statistics__\n",
    "> precision: <br>\n",
    "> recall: <br>\n",
    "> f1_score: <br>\n",
    "> accuracy: <br>\n",
    "\n",
    "\n",
    "__4. Requested Classification Statistics__\n",
    "> precision: <br>\n",
    "> recall: <br>\n",
    "> f1_score: <br>\n",
    "> accuracy: \n",
    "\n",
    "__5. Requested Classification Statistics__\n",
    "> precision: <br>\n",
    "> recall: <br>\n",
    "> f1_score: <br>\n",
    "> accuracy: <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset  (sklearn.datasets.load_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris flower data set or Fishers Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.\n",
    "\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
    "\n",
    "The iris data set is widely used as a beginner's dataset for machine learning purposes. The dataset is included in the machine learning package Scikit-learn, so that users can access it without having to find a source for it. The following python code illustrates usage.\n",
    "\n",
    "```\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spambase Dataset  (spambase.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of 4601 instances of data, each with 57 features and a class label designating if the sample is spam or not.\n",
    "The features are _real valued_ and are described in much detail here:\n",
    "\n",
    "> https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names\n",
    "\n",
    "Data obtained from:  https://archive.ics.uci.edu/ml/datasets/Spambase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from pprint import pprint\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1__ Consider the following set of training examples for an unknown target function:  $(x_1, x_2)\\rightarrow y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y x1 x2  Count\n",
       "0  1  T  T      3\n",
       "1  1  T  F      4\n",
       "2  1  F  T      4\n",
       "3  1  F  F      1\n",
       "4  0  T  T      0\n",
       "5  0  T  F      1\n",
       "6  0  F  T      3\n",
       "7  0  F  F      5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theory_data1 = pd.DataFrame({\n",
    "   \"Y\" : ['+', '+', '+', '+', '-', '-', '-', '-'], \n",
    "    \"x1\" : ['T', 'T', 'F', 'F', 'T', 'T', 'F', 'F'],\n",
    "    \"x2\" : ['T', 'F', 'T', 'F', 'T', 'F', 'T', 'F'],\n",
    "    \"Count\" : [3, 4, 4, 1, 0, 1, 3, 5]\n",
    "})\n",
    "theory_data1['Y'] = abs(theory_data1['Y'].astype('category').cat.codes - 1)\n",
    "theory_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What is the sample entropy, $H(Y)$ from this training data (using log base 2) (2pts)? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9852281360342515"
      ]
     },
     "execution_count": 844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy=-((9/21)*math.log2(9/21)+(12/21)*math.log2(12/21))\n",
    "entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p(0)=9/21\n",
    "p(1)=12/21 from the above data.\n",
    "Entropy = sum of (-pi*log2(pi))\n",
    "        = -((9/21)*log2(9/21)+(12/21)*log2(12/21))\n",
    "      H(Y) = 0.9852281360342515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. item What are the information gains for branching on variables $x_1$ and $x_2$ (2pts)? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20707216883794147"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8/21*((-7/8)*math.log2(7/8)+(-1/8)*math.log2(1/8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5950512314951136"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13/21*((-5/13)*math.log2(5/13)+(-8/13)*math.log2(8/13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig1=entropy-(0.20707216883794147+0.5950512314951136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18310473570119645"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain= H(Y)-H(Y/X1)\n",
    "= .985229 - (-8/21*((-7/8)*math.log2(7/8)+(-1/8)*math.log2(1/8)))-13/21*((-5/13)*math.log2(5/13)+(-8/13)*math.log2(8/13))\n",
    "=0.18310473570119645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4196623329669965"
      ]
     },
     "execution_count": 856,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10/21*((-7/10)*math.log2(7/10)+(-3/10)*math.log2(3/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5206824917260249"
      ]
     },
     "execution_count": 857,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11/21*((-5/11)*math.log2(5/11)+(-6/11)*math.log2(6/11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig2=entropy-(0.4196623329669965+0.5206824917260249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04488331134123014"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ig2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain= H(Y)-H(Y/X2)\n",
    "= .985229 - (-10/21*((-7/10)*math.log2(7/10)+(-3/10)*math.log2(3/10)))-13/21*((11/21*((-5/11)*math.log2(5/11)+(-6/11)*math.log2(6/11))\n",
    "=0.04488331134123014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. item Draw the deicion tree that would be learned by the ID3 algorithm without pruning from this training data (3pts)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__2__ \n",
    "\n",
    "We decided that maybe we can use the number of characters and the average word length an essay to determine if the student should get an $A$ in a class or not.  Below are five samples of this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of Chars</th>\n",
       "      <th>Average Word Length</th>\n",
       "      <th>Give an A</th>\n",
       "      <th>bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216</td>\n",
       "      <td>5.68</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>4.78</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302</td>\n",
       "      <td>2.31</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>3.16</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>393</td>\n",
       "      <td>4.20</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # of Chars  Average Word Length Give an A  bool\n",
       "0         216                 5.68       Yes     1\n",
       "1          69                 4.78       Yes     1\n",
       "2         302                 2.31        No     0\n",
       "3          60                 3.16       Yes     1\n",
       "4         393                 4.20        No     0"
      ]
     },
     "execution_count": 900,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theory_data2 = pd.DataFrame({\n",
    "    \"# of Chars\" : [216, 69, 302, 60, 393],\n",
    "    \"Average Word Length\" : [5.68, 4.78, 2.31, 3.16, 4.2],\n",
    "    \"Give an A\" : ['Yes', 'Yes', 'No', 'Yes', 'No']})\n",
    "theory_data2['bool'] = theory_data2['Give an A'].astype('category').cat.codes\n",
    "theory_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What are the class priors, $P(A=Yes), P(A=No)$? (2pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(A=Yes)=3/5 P(A=no)=2/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Find the parameters of the Gaussians necessary to do Gaussian Naive Bayes classification on this decision to give an A or not.  Standardize the features first over all the data together so that there is no unfair bias towards the features of different scales (2pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardise(string1):\n",
    "    m1=np.mean(string1)\n",
    "    std1=np.std(string1)\n",
    "    string2=(string1-m1)/std1\n",
    "    return string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=np.array(theory_data2[\"bool\"])\n",
    "theory_data2=theory_data2.drop([\"Give an A\"],axis=1)\n",
    "theory_data2=theory_data2.drop([\"bool\"],axis=1)\n",
    "X_train=theory_data2.apply(Standardise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]], dtype=int8)"
      ]
     },
     "execution_count": 903,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train=Y_train.reshape(-1,1)\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zero= X_train[Y_train == 0]\n",
    "df_one = X_train[Y_train==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of Chars</th>\n",
       "      <th>Average Word Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.723720</td>\n",
       "      <td>-1.447277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.424342</td>\n",
       "      <td>0.146752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # of Chars  Average Word Length\n",
       "2    0.723720            -1.447277\n",
       "4    1.424342             0.146752"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of Chars</th>\n",
       "      <th>Average Word Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.061593</td>\n",
       "      <td>1.394987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.070181</td>\n",
       "      <td>0.635925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.139473</td>\n",
       "      <td>-0.730386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # of Chars  Average Word Length\n",
       "0    0.061593             1.394987\n",
       "1   -1.070181             0.635925\n",
       "3   -1.139473            -0.730386"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(string3):\n",
    "    string4=np.mean(string3)\n",
    "    return string4\n",
    "def method1(string3):\n",
    "    string4=np.var(string3)\n",
    "    return string4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zero1=df_zero.apply(method)\n",
    "df_zero2=df_zero.apply(method1)\n",
    "df_one1=df_one.apply(method)\n",
    "df_one2=df_one.apply(method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def norm1(X,mean,var):\n",
    "    x2=2*var\n",
    "    x3=(X-mean)**2\n",
    "    x4=2*math.pi*var\n",
    "    Ck=(1/np.sqrt(x4))*(np.exp(-x3/x2))\n",
    "    return Ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "X_one=norm1(X_train,df_one1,df_one2)\n",
    "X_zero=norm1(X_train,df_zero1,df_zero2)\n",
    "priority0 = len(df_zero) / len(X_train)\n",
    "priority1 = len(df_one) / len(X_train)\n",
    "print(priority0)\n",
    "print(priority1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   # of Chars  Average Word Length\n",
      "0    0.160358             0.149728\n",
      "1    0.353501             0.265074\n",
      "2    0.014237             0.027646\n",
      "3    0.323439             0.113372\n",
      "4    0.000227             0.258097\n",
      "     # of Chars  Average Word Length\n",
      "0  6.994277e-03             0.007440\n",
      "1  3.334723e-09             0.054451\n",
      "2  2.762924e-01             0.121439\n",
      "3  9.744360e-10             0.199209\n",
      "4  2.762924e-01             0.121439\n"
     ]
    }
   ],
   "source": [
    "a_prob=X_one*priority1\n",
    "nota_prob=X_zero*priority0\n",
    "print(a_prob)\n",
    "print(nota_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prior = 1/n_class\n",
    "\n",
    "final_probabilities = []\n",
    "for i in k:\n",
    "  class_prob = np.prod(i) * prior\n",
    "  final_probabilities.append(class_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Chars             5.933885e-08\n",
      "Average Word Length    3.210620e-05\n",
      "dtype: float64\n",
      "# of Chars             1.734977e-21\n",
      "Average Word Length    1.190158e-06\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(np.prod(a_prob))\n",
    "print(np.prod(nota_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Using your response from the prior question, determine if an essay with 242 characters and an average word length of 4.56 should get an A or not (3pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__3__\n",
    "\n",
    "Consider the following questions pertaining to a k-Nearest Neighbors algorithm (1pt):\n",
    "\n",
    "a. How could you use a _validation set_ to determine the user-defined parameter $k$?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train and test a _Logistic Regression Classifier_ to classify flowers from the Iris Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download import the data from sklearn.datasets.  As mentioned in the Datasets area,  The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.  We will map this into a binary classification problem between Iris setosa versus Iris virgincia and versicolor.  We will use just the first 2 features, width and length of the sepals.  \n",
    "\n",
    "For this part, we will be practicing gradient descent with logistic regression.\n",
    "\n",
    "Use the following code to load the data, and binarize the target values.\n",
    "\n",
    "```\n",
    "iris = skdata.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = (iris.target != 0) * 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write a script that:__\n",
    "\n",
    "\n",
    "1. Reads in the data with the script above.\n",
    "2. Standardizes the data using the mean and standard deviation\n",
    "3. Initialize the parameters of $\\theta$ using random values in the range [-1, 1]\n",
    "4. Do __batch__ gradient descent\n",
    "5.  Terminate when absolute value change in the loss on the data is less than $2^{-23}$, or after $10,000$ iterations have passed (whichever occurs first).\n",
    "6.  Use a learning rate $\\eta=0.01$.\n",
    "7. While the termination criteria (mentioned above in the implementation details) hasn't been met <br>\n",
    " a. Compute the loss of the data using the logistic regression cost<br>\n",
    " b. Update each parameter using __batch__ gradient descent<br>\n",
    " \n",
    "Plot the data and the decision boundary using matplotlib.  Verify your solution with the LogisticRegression sklearn method.\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression(penalty='none',solver='lbfgs',max_iter=10000)\n",
    "lgr.fit(X,y)\n",
    "```\n",
    "\n",
    "In your writeup, present the thetas from gradient descent that minimize the loss function as well as plots of your method versus the built in LogisticRegression method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)'],\""
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)'],'''\n",
    "#iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :2]\n",
    "y = (iris.target != 0) * 1\n",
    "y=y.reshape(-1,1)\n",
    "#print(X)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardise(string1):\n",
    "    m1=np.mean(string1)\n",
    "    std1=np.std(string1)\n",
    "    string2=(string1-m1)/std1\n",
    "    return string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=Standardise(X[:,0])\n",
    "X2=Standardise(X[:,1])\n",
    "ones = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
    "X1=X1.reshape(-1,1)\n",
    "X2=X2.reshape(-1,1)\n",
    "X3=np.c_[ones,X1,X2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "seed(42)\n",
    "size=X3.shape[1]\n",
    "size\n",
    "values = rand(size)\n",
    "theta=np.array(values).reshape(-1,1)\n",
    "theta1=np.array(values).reshape(-1,1)\n",
    "theta2=theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate gradient\n",
    "theta = theta -gradientTheta\n",
    "while(True):\n",
    "    calculate gradient\n",
    "    newTheta = theta - gradient\n",
    "    if gradient is very close to zero and abs(newTheta-Theta) is very close to zero:\n",
    "       break from loop # (The algorithm has converged)\n",
    "    theta = newTheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss1(gx, y):\n",
    "    cost=0\n",
    "    for i, j in zip(gx,y):\n",
    "        if ( i!= 1 and i != 0 ):\n",
    "            cost = cost + j*np.log(i)+(1-j)*np.log(1-i)\n",
    "\n",
    "    return np.mean(cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.17513889],\n",
       "       [ 3.81031692],\n",
       "       [-2.56300942]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=0\n",
    "count=0\n",
    "eta=0.01\n",
    "iter = 10000\n",
    "train_error = [1]\n",
    "while count<iter:   \n",
    "    gx1=1/(1+np.exp(-X3@theta))\n",
    "    grad=X3.transpose() @ (gx1 - y)\n",
    "    theta=theta-((eta/X3.shape[0])*grad)  \n",
    "    #print(theta)\n",
    "    cost2=loss1(gx1,y)\n",
    "    train_error.append(cost2)\n",
    "    if abs(train_error[-2]-train_error[-1])<=2**-23:\n",
    "        break\n",
    "    count=count+1\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f82c8fb5190>]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(theta2, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brindakulkarni/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, penalty='none')"
      ]
     },
     "execution_count": 948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression(penalty='none',solver='lbfgs',max_iter=10000)\n",
    "lgr.fit(X3,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 19.97028527,  60.36395919, -28.8049862 ]])"
      ]
     },
     "execution_count": 949,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Logistic Regression Spam Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train and test a _Logistic Regression Classifier_ to classifiy Spam or Not from the Spambase Dataset.\n",
    "\n",
    "First download the dataset _spambase.data_ from Blackboard.  As mentioned in the Datasets area, this dataset contains 4601 rows of data, each with 57 continuous valued features followed by a binary class label (0=not-spam, 1=spam).  There is no header information in this file and the data is comma separated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write a script that:__\n",
    "\n",
    "\n",
    "1. Reads in the data.\n",
    "2. Randomizes the data.\n",
    "3. Selects the first 2/3 (round up) of the data for training and the remaining for testing (you may use  __sklearn train\\_test\\_split__ for this part)\n",
    "4. Standardizes the data (except for the last column of course) using the training data\n",
    "5. Initialize the parameters of $\\theta$ using random values in the range [-1, 1]\n",
    "6.  Do _batch gradient descent_\n",
    "7. Terminate when absolute value change in the loss on the data is less than $2^{-23}$, or after $1,500$ iterations have passed (whichever occurs first, this will likely be a slow process).\n",
    "8. Use a learning rate $\\eta=0.01$.\n",
    "9. Classify each testing sample using the model and choosing the class label based on which class probability is higher.\n",
    "10. Computes the following statistics using the testing data results:<br>\n",
    "[lecture 3 b] <br>\n",
    " a. Precision<br>\n",
    " b. Recall<br>\n",
    " c. F-measure<br>\n",
    " d. Accuracy<br>\n",
    " \n",
    "__Implementation Details__\n",
    "\n",
    "1. Seed the random number generate with zero prior to randomizing the data\n",
    "2. There are a lot of $\\theta$s and this will likely be a slow process\n",
    "\n",
    "__In your report you will need__\n",
    "\n",
    "1. The statistics requested for your Logistic classifier run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1=pd.read_csv(\"spambase.data\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=df1[57]\n",
    "X=df1.drop([57],axis=1)\n",
    "Y=np.array(Y)\n",
    "Y=Y.reshape(-1,1)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X,Y,test_size=0.34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3036   1565\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),\" \",len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardise(string1):\n",
    "    m1=np.mean(string1)\n",
    "    std1=np.std(string1)\n",
    "    string2=(string1-m1)/std1\n",
    "    return string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.apply(Standardise)\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3036, 58)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = np.ones(X_train.shape[0]).reshape(X_train.shape[0], 1)\n",
    "X_train=np.concatenate((ones,X_train),1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "seed(42)\n",
    "size=X_train.shape[1]\n",
    "values = rand(size)\n",
    "theta=np.array(values).reshape(-1,1)\n",
    "theta1=np.array(values).reshape(-1,1)\n",
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9996303 ],\n",
       "       [0.00693386],\n",
       "       [0.9999065 ],\n",
       "       ...,\n",
       "       [0.03476937],\n",
       "       [0.45202125],\n",
       "       [0.0055623 ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gx=1/(1+np.exp(-X_train@theta))\n",
    "gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-d299603e6eba>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  gx2=np.log(1-gx)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-3.69768364e-04],\n",
       "       [-4.97133903e+00],\n",
       "       [-9.35074504e-05],\n",
       "       ...,\n",
       "       [-3.35901834e+00],\n",
       "       [-7.94026092e-01],\n",
       "       [-5.19174370e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gx1=np.log(gx)\n",
    "gx2=np.log(1-gx)\n",
    "gx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss1(gx, y):\n",
    "    cost=0\n",
    "    for i, j in zip(gx,y):\n",
    "        if ( i!= 1 and i != 0 ):\n",
    "            cost += j*np.log(i)+(1-j)*np.log(1-i)\n",
    "\n",
    "    return np.mean(cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.49792168],\n",
       "       [-0.12033906],\n",
       "       [-0.12664462],\n",
       "       [ 0.13296111],\n",
       "       [ 0.37495036],\n",
       "       [ 0.35708362],\n",
       "       [ 0.17670686],\n",
       "       [ 1.08024375],\n",
       "       [ 0.30829383],\n",
       "       [ 0.32568932],\n",
       "       [ 0.04978401],\n",
       "       [ 0.09051493],\n",
       "       [-0.14363676],\n",
       "       [-0.04696918],\n",
       "       [ 0.11327776],\n",
       "       [ 0.3538238 ],\n",
       "       [ 0.39877652],\n",
       "       [ 0.38564674],\n",
       "       [ 0.21212101],\n",
       "       [ 0.17347728],\n",
       "       [ 0.53586228],\n",
       "       [ 0.24005825],\n",
       "       [ 0.43469973],\n",
       "       [ 0.97050643],\n",
       "       [ 0.47923177],\n",
       "       [-0.93761023],\n",
       "       [-0.75891843],\n",
       "       [-0.68895864],\n",
       "       [ 0.14234934],\n",
       "       [-0.35847343],\n",
       "       [-0.27614312],\n",
       "       [-0.24338315],\n",
       "       [-0.43435494],\n",
       "       [-0.35589355],\n",
       "       [ 0.40896515],\n",
       "       [-0.23828744],\n",
       "       [ 0.15636356],\n",
       "       [-0.13030599],\n",
       "       [-0.07754049],\n",
       "       [-0.23110903],\n",
       "       [-0.22135012],\n",
       "       [-0.28164779],\n",
       "       [-0.64299131],\n",
       "       [-0.14763768],\n",
       "       [-0.31326566],\n",
       "       [-0.62455775],\n",
       "       [-0.45438662],\n",
       "       [-0.18363917],\n",
       "       [-0.26554588],\n",
       "       [-0.31694926],\n",
       "       [-0.00395129],\n",
       "       [-0.06864494],\n",
       "       [ 0.41116475],\n",
       "       [ 1.30262568],\n",
       "       [ 0.49397032],\n",
       "       [ 1.18194084],\n",
       "       [ 0.55580615],\n",
       "       [ 0.31243488]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=0\n",
    "count=0\n",
    "eta=0.01\n",
    "iter = 10000\n",
    "train_error = [1]\n",
    "while count<iter:   \n",
    "    gx=1/(1+np.exp(-X_train@theta))\n",
    "    grad=X_train.transpose() @ (gx - Y_train)\n",
    "    theta=theta-((eta/X_train.shape[0])*grad)  \n",
    "    #print(theta)\n",
    "    cost2=loss1(gx,Y_train)\n",
    "    train_error.append(cost2)\n",
    "    if abs(train_error[-2]-train_error[-1])<=2**-23:\n",
    "        break\n",
    "    count=count+1\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Naive Bayes Classifier\n",
    "\n",
    "Let's train and test a _Naive Bayes Classifier_ to classifiy Spam or Not from the Spambase Dataset.\n",
    "\n",
    "First download the dataset _spambase.data_ from Blackboard.  As mentioned in the Datasets area, this dataset contains 4601 rows of data, each with 57 continuous valued features followed by a binary class label (0=not-spam, 1=spam).  There is no header information in this file and the data is comma separated.  As always, your code should work on any dataset that lacks header information and has several comma-separated continuous-valued features followed by a class id $\\in {0,1}$.\n",
    "\n",
    "__Write a script that:__\n",
    "\n",
    "1. Reads in the data.\n",
    "2. Randomizes the data.\n",
    "3. Selects the first 2/3 (round up) of the data for training and the remaining for testing\n",
    "4. Standardizes the data (except for the last column of course) using the training data\n",
    "5. Divides the training data into two groups: Spam samples, Non-Spam samples.\n",
    "6. Creates Gaussian models for each feature for each class.\n",
    "7. Classify each testing sample using these models and choosing the class label based on which class probability is higher.\n",
    "8. Computes the following statistics using the testing data results:<br>\n",
    " a. Precision<br>\n",
    " b. Recall<br>\n",
    " c. F-measure<br>\n",
    " d. Accuracy<br>\n",
    " \n",
    "__Implementation Details__\n",
    "\n",
    "1.  Seed the random number generate with zero prior to randomizing the data\n",
    "2.  If  you  decide  to  work  in  log  space,  realize  that  python  interprets  0log0  as  inf. You  should identify this situation and either add an EPS (very small positive number) or add a very largenegative number to the log sum.\n",
    "\n",
    "__In your report you will need:__\n",
    "\n",
    "1.  The statistics requested for your Naive Bayes classifier run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1=pd.read_csv(\"spambase.data\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = df1.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training set: 3037\n",
      "No. of testing set: 1564\n"
     ]
    }
   ],
   "source": [
    "train = df1.sample(frac=0.66, random_state=42)\n",
    "test = df1.drop(train.index)\n",
    "print(f\"No. of training set: {train.shape[0]}\")\n",
    "print(f\"No. of testing set: {test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardise(string1):\n",
    "    m1=np.mean(string1)\n",
    "    std1=np.std(string1)\n",
    "    string2=(string1-m1)/std1\n",
    "    return string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=np.array(train[57])\n",
    "Y_train=Y_train.reshape(-1,1)\n",
    "train=train.drop([57],axis=1)\n",
    "train=train.apply(Standardise)\n",
    "Y_test=np.array(test[57])\n",
    "Y_test=Y_test.reshape(-1,1)\n",
    "test=test.drop([57],axis=1)\n",
    "test=test.apply(Standardise)\n",
    "#train[57]=Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zero= train[Y_train == 0]\n",
    "index1 = df_zero.index\n",
    "index1.name = \"Non Spam Samples\"\n",
    "df_one = train[Y_train==1]\n",
    "index = df_one.index\n",
    "index.name = \"Spam Samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(string3):\n",
    "    string4=np.mean(string3)\n",
    "    return string4\n",
    "def method1(string3):\n",
    "    string4=np.var(string3)\n",
    "    return string4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zero1=df_zero.apply(method)\n",
    "df_zero2=df_zero.apply(method1)\n",
    "df_one1=df_one.apply(method)\n",
    "df_one2=df_one.apply(method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_zero2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def norm1(X,mean,var):\n",
    "    x2=2*var\n",
    "    x3=(X-mean)**2\n",
    "    x4=2*math.pi*var\n",
    "    Ck=(1/np.sqrt(x4))*(np.exp(-x3/x2))\n",
    "    return Ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6042146855449456\n",
      "0.3957853144550543\n"
     ]
    }
   ],
   "source": [
    "X_one=norm1(train,df_one1,df_one2)\n",
    "X_zero=norm1(train,df_zero1,df_zero2)\n",
    "priority0 = len(df_zero) / len(train)\n",
    "priority1 = len(df_one) / len(train)\n",
    "print(priority0)\n",
    "print(priority1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>0.423867</td>\n",
       "      <td>1.381900</td>\n",
       "      <td>0.296082</td>\n",
       "      <td>0.250618</td>\n",
       "      <td>0.293189</td>\n",
       "      <td>0.325384</td>\n",
       "      <td>0.240290</td>\n",
       "      <td>0.298267</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.340766</td>\n",
       "      <td>...</td>\n",
       "      <td>2.703578</td>\n",
       "      <td>1.135519</td>\n",
       "      <td>0.353830</td>\n",
       "      <td>1.176906e+00</td>\n",
       "      <td>0.318610</td>\n",
       "      <td>0.267858</td>\n",
       "      <td>0.276069</td>\n",
       "      <td>0.250797</td>\n",
       "      <td>0.246756</td>\n",
       "      <td>0.302050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>0.378700</td>\n",
       "      <td>1.381900</td>\n",
       "      <td>0.008076</td>\n",
       "      <td>0.250618</td>\n",
       "      <td>0.117588</td>\n",
       "      <td>0.293578</td>\n",
       "      <td>0.011559</td>\n",
       "      <td>0.277289</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.340766</td>\n",
       "      <td>...</td>\n",
       "      <td>2.703578</td>\n",
       "      <td>1.104511</td>\n",
       "      <td>0.314856</td>\n",
       "      <td>1.176906e+00</td>\n",
       "      <td>0.366038</td>\n",
       "      <td>0.240734</td>\n",
       "      <td>0.276069</td>\n",
       "      <td>0.251388</td>\n",
       "      <td>0.245674</td>\n",
       "      <td>0.266389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.378700</td>\n",
       "      <td>0.279717</td>\n",
       "      <td>0.296082</td>\n",
       "      <td>0.250618</td>\n",
       "      <td>0.354427</td>\n",
       "      <td>0.293578</td>\n",
       "      <td>0.240290</td>\n",
       "      <td>0.277289</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.321200</td>\n",
       "      <td>...</td>\n",
       "      <td>2.703578</td>\n",
       "      <td>1.104511</td>\n",
       "      <td>0.314856</td>\n",
       "      <td>1.176906e+00</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>0.240734</td>\n",
       "      <td>0.276069</td>\n",
       "      <td>0.249861</td>\n",
       "      <td>0.242138</td>\n",
       "      <td>0.275005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3421</th>\n",
       "      <td>0.378700</td>\n",
       "      <td>1.381900</td>\n",
       "      <td>0.296082</td>\n",
       "      <td>0.250618</td>\n",
       "      <td>0.293189</td>\n",
       "      <td>0.293578</td>\n",
       "      <td>0.240290</td>\n",
       "      <td>0.277289</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.340766</td>\n",
       "      <td>...</td>\n",
       "      <td>2.703578</td>\n",
       "      <td>1.104511</td>\n",
       "      <td>0.032264</td>\n",
       "      <td>5.977735e-47</td>\n",
       "      <td>0.316348</td>\n",
       "      <td>0.240734</td>\n",
       "      <td>0.276069</td>\n",
       "      <td>0.249986</td>\n",
       "      <td>0.239991</td>\n",
       "      <td>0.263548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414</th>\n",
       "      <td>0.378700</td>\n",
       "      <td>1.381900</td>\n",
       "      <td>0.138409</td>\n",
       "      <td>0.250618</td>\n",
       "      <td>0.293189</td>\n",
       "      <td>0.018062</td>\n",
       "      <td>0.240290</td>\n",
       "      <td>0.277289</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.245448</td>\n",
       "      <td>...</td>\n",
       "      <td>2.703578</td>\n",
       "      <td>1.104511</td>\n",
       "      <td>0.314856</td>\n",
       "      <td>1.176906e+00</td>\n",
       "      <td>0.373259</td>\n",
       "      <td>0.240734</td>\n",
       "      <td>0.276069</td>\n",
       "      <td>0.249956</td>\n",
       "      <td>0.243364</td>\n",
       "      <td>0.265561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>0.378700</td>\n",
       "      <td>1.563959</td>\n",
       "      <td>0.296082</td>\n",
       "      <td>0.250618</td>\n",
       "      <td>0.374249</td>\n",
       "      <td>0.301893</td>\n",
       "      <td>0.240290</td>\n",
       "      <td>0.277289</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.382456</td>\n",
       "      <td>...</td>\n",
       "      <td>2.703578</td>\n",
       "      <td>1.104511</td>\n",
       "      <td>0.343783</td>\n",
       "      <td>1.176906e+00</td>\n",
       "      <td>0.316348</td>\n",
       "      <td>0.249060</td>\n",
       "      <td>0.276069</td>\n",
       "      <td>0.250358</td>\n",
       "      <td>0.241719</td>\n",
       "      <td>0.284456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>0.266917</td>\n",
       "      <td>1.381900</td>\n",
       "      <td>0.296082</td>\n",
       "      <td>0.250618</td>\n",
       "      <td>0.338133</td>\n",
       "      <td>0.044573</td>\n",
       "      <td>0.240290</td>\n",
       "      <td>0.277289</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.399229</td>\n",
       "      <td>...</td>\n",
       "      <td>2.703578</td>\n",
       "      <td>1.104511</td>\n",
       "      <td>0.314856</td>\n",
       "      <td>1.176906e+00</td>\n",
       "      <td>0.368866</td>\n",
       "      <td>0.240734</td>\n",
       "      <td>0.276069</td>\n",
       "      <td>0.252235</td>\n",
       "      <td>0.255430</td>\n",
       "      <td>0.285568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.378700</td>\n",
       "      <td>1.165277</td>\n",
       "      <td>0.417745</td>\n",
       "      <td>0.250618</td>\n",
       "      <td>0.383402</td>\n",
       "      <td>0.293578</td>\n",
       "      <td>0.240290</td>\n",
       "      <td>0.277289</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.340766</td>\n",
       "      <td>...</td>\n",
       "      <td>2.703578</td>\n",
       "      <td>1.104511</td>\n",
       "      <td>0.350967</td>\n",
       "      <td>1.176906e+00</td>\n",
       "      <td>0.376193</td>\n",
       "      <td>0.240734</td>\n",
       "      <td>0.276069</td>\n",
       "      <td>0.250905</td>\n",
       "      <td>0.255838</td>\n",
       "      <td>0.290475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>0.378700</td>\n",
       "      <td>1.381900</td>\n",
       "      <td>0.296082</td>\n",
       "      <td>0.250618</td>\n",
       "      <td>0.293189</td>\n",
       "      <td>0.293578</td>\n",
       "      <td>0.240290</td>\n",
       "      <td>0.277289</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.241689</td>\n",
       "      <td>...</td>\n",
       "      <td>2.703578</td>\n",
       "      <td>1.104511</td>\n",
       "      <td>0.232111</td>\n",
       "      <td>1.176906e+00</td>\n",
       "      <td>0.316348</td>\n",
       "      <td>0.240734</td>\n",
       "      <td>0.276069</td>\n",
       "      <td>0.250814</td>\n",
       "      <td>0.242552</td>\n",
       "      <td>0.276327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>0.376008</td>\n",
       "      <td>1.452351</td>\n",
       "      <td>0.296082</td>\n",
       "      <td>0.250618</td>\n",
       "      <td>0.383186</td>\n",
       "      <td>0.316621</td>\n",
       "      <td>0.240290</td>\n",
       "      <td>0.256981</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.340766</td>\n",
       "      <td>...</td>\n",
       "      <td>2.703578</td>\n",
       "      <td>1.104511</td>\n",
       "      <td>0.314856</td>\n",
       "      <td>1.176906e+00</td>\n",
       "      <td>0.316348</td>\n",
       "      <td>0.240734</td>\n",
       "      <td>0.276069</td>\n",
       "      <td>0.249798</td>\n",
       "      <td>0.240431</td>\n",
       "      <td>0.272131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1564 rows √ó 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "3507  0.423867  1.381900  0.296082  0.250618  0.293189  0.325384  0.240290   \n",
       "933   0.378700  1.381900  0.008076  0.250618  0.117588  0.293578  0.011559   \n",
       "192   0.378700  0.279717  0.296082  0.250618  0.354427  0.293578  0.240290   \n",
       "3421  0.378700  1.381900  0.296082  0.250618  0.293189  0.293578  0.240290   \n",
       "4414  0.378700  1.381900  0.138409  0.250618  0.293189  0.018062  0.240290   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1614  0.378700  1.563959  0.296082  0.250618  0.374249  0.301893  0.240290   \n",
       "1132  0.266917  1.381900  0.296082  0.250618  0.338133  0.044573  0.240290   \n",
       "97    0.378700  1.165277  0.417745  0.250618  0.383402  0.293578  0.240290   \n",
       "2072  0.378700  1.381900  0.296082  0.250618  0.293189  0.293578  0.240290   \n",
       "3509  0.376008  1.452351  0.296082  0.250618  0.383186  0.316621  0.240290   \n",
       "\n",
       "            7         8         9   ...        47        48        49  \\\n",
       "3507  0.298267  0.288782  0.340766  ...  2.703578  1.135519  0.353830   \n",
       "933   0.277289  0.288782  0.340766  ...  2.703578  1.104511  0.314856   \n",
       "192   0.277289  0.288782  0.321200  ...  2.703578  1.104511  0.314856   \n",
       "3421  0.277289  0.288782  0.340766  ...  2.703578  1.104511  0.032264   \n",
       "4414  0.277289  0.288782  0.245448  ...  2.703578  1.104511  0.314856   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1614  0.277289  0.288782  0.382456  ...  2.703578  1.104511  0.343783   \n",
       "1132  0.277289  0.288782  0.399229  ...  2.703578  1.104511  0.314856   \n",
       "97    0.277289  0.288782  0.340766  ...  2.703578  1.104511  0.350967   \n",
       "2072  0.277289  0.288782  0.241689  ...  2.703578  1.104511  0.232111   \n",
       "3509  0.256981  0.288782  0.340766  ...  2.703578  1.104511  0.314856   \n",
       "\n",
       "                50        51        52        53        54        55        56  \n",
       "3507  1.176906e+00  0.318610  0.267858  0.276069  0.250797  0.246756  0.302050  \n",
       "933   1.176906e+00  0.366038  0.240734  0.276069  0.251388  0.245674  0.266389  \n",
       "192   1.176906e+00  0.331092  0.240734  0.276069  0.249861  0.242138  0.275005  \n",
       "3421  5.977735e-47  0.316348  0.240734  0.276069  0.249986  0.239991  0.263548  \n",
       "4414  1.176906e+00  0.373259  0.240734  0.276069  0.249956  0.243364  0.265561  \n",
       "...            ...       ...       ...       ...       ...       ...       ...  \n",
       "1614  1.176906e+00  0.316348  0.249060  0.276069  0.250358  0.241719  0.284456  \n",
       "1132  1.176906e+00  0.368866  0.240734  0.276069  0.252235  0.255430  0.285568  \n",
       "97    1.176906e+00  0.376193  0.240734  0.276069  0.250905  0.255838  0.290475  \n",
       "2072  1.176906e+00  0.316348  0.240734  0.276069  0.250814  0.242552  0.276327  \n",
       "3509  1.176906e+00  0.316348  0.240734  0.276069  0.249798  0.240431  0.272131  \n",
       "\n",
       "[1564 rows x 57 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1=norm1(test,df_one1,df_one2)\n",
    "X_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>0.383826</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.370577</td>\n",
       "      <td>2.608648</td>\n",
       "      <td>0.413478</td>\n",
       "      <td>0.485956</td>\n",
       "      <td>1.846038e+00</td>\n",
       "      <td>0.433179</td>\n",
       "      <td>0.511846</td>\n",
       "      <td>0.389569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30973</td>\n",
       "      <td>0.317708</td>\n",
       "      <td>0.424588</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.450252</td>\n",
       "      <td>0.245804</td>\n",
       "      <td>0.675169</td>\n",
       "      <td>5.433926</td>\n",
       "      <td>3.208540</td>\n",
       "      <td>0.355367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>0.372270</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>2.608648</td>\n",
       "      <td>0.033407</td>\n",
       "      <td>0.488592</td>\n",
       "      <td>3.203748e-92</td>\n",
       "      <td>0.634675</td>\n",
       "      <td>0.511846</td>\n",
       "      <td>0.389569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30973</td>\n",
       "      <td>0.313693</td>\n",
       "      <td>0.325348</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.420278</td>\n",
       "      <td>1.695053</td>\n",
       "      <td>0.675169</td>\n",
       "      <td>4.024478</td>\n",
       "      <td>2.951400</td>\n",
       "      <td>0.597052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.372270</td>\n",
       "      <td>0.298641</td>\n",
       "      <td>0.370577</td>\n",
       "      <td>2.608648</td>\n",
       "      <td>0.260602</td>\n",
       "      <td>0.488592</td>\n",
       "      <td>1.846038e+00</td>\n",
       "      <td>0.634675</td>\n",
       "      <td>0.511846</td>\n",
       "      <td>0.252184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30973</td>\n",
       "      <td>0.313693</td>\n",
       "      <td>0.325348</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.197885</td>\n",
       "      <td>1.695053</td>\n",
       "      <td>0.675169</td>\n",
       "      <td>5.990254</td>\n",
       "      <td>1.828165</td>\n",
       "      <td>0.610941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3421</th>\n",
       "      <td>0.372270</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.370577</td>\n",
       "      <td>2.608648</td>\n",
       "      <td>0.413478</td>\n",
       "      <td>0.488592</td>\n",
       "      <td>1.846038e+00</td>\n",
       "      <td>0.634675</td>\n",
       "      <td>0.511846</td>\n",
       "      <td>0.389569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30973</td>\n",
       "      <td>0.313693</td>\n",
       "      <td>0.017127</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.450043</td>\n",
       "      <td>1.695053</td>\n",
       "      <td>0.675169</td>\n",
       "      <td>6.034251</td>\n",
       "      <td>1.208294</td>\n",
       "      <td>0.590701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414</th>\n",
       "      <td>0.372270</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.075568</td>\n",
       "      <td>2.608648</td>\n",
       "      <td>0.413478</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>1.846038e+00</td>\n",
       "      <td>0.634675</td>\n",
       "      <td>0.511846</td>\n",
       "      <td>0.173804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30973</td>\n",
       "      <td>0.313693</td>\n",
       "      <td>0.325348</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.399957</td>\n",
       "      <td>1.695053</td>\n",
       "      <td>0.675169</td>\n",
       "      <td>6.026799</td>\n",
       "      <td>2.227830</td>\n",
       "      <td>0.595282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>0.372270</td>\n",
       "      <td>0.313919</td>\n",
       "      <td>0.370577</td>\n",
       "      <td>2.608648</td>\n",
       "      <td>0.416547</td>\n",
       "      <td>0.218483</td>\n",
       "      <td>1.846038e+00</td>\n",
       "      <td>0.634675</td>\n",
       "      <td>0.511846</td>\n",
       "      <td>0.404618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30973</td>\n",
       "      <td>0.313693</td>\n",
       "      <td>0.391710</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.450043</td>\n",
       "      <td>1.643933</td>\n",
       "      <td>0.675169</td>\n",
       "      <td>5.959765</td>\n",
       "      <td>1.697149</td>\n",
       "      <td>0.613249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>0.214007</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.370577</td>\n",
       "      <td>2.608648</td>\n",
       "      <td>0.231212</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>1.846038e+00</td>\n",
       "      <td>0.634675</td>\n",
       "      <td>0.511846</td>\n",
       "      <td>0.374232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30973</td>\n",
       "      <td>0.313693</td>\n",
       "      <td>0.325348</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.292305</td>\n",
       "      <td>1.695053</td>\n",
       "      <td>0.675169</td>\n",
       "      <td>1.448181</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.612290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.372270</td>\n",
       "      <td>0.312734</td>\n",
       "      <td>0.340949</td>\n",
       "      <td>2.608648</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.488592</td>\n",
       "      <td>1.846038e+00</td>\n",
       "      <td>0.634675</td>\n",
       "      <td>0.511846</td>\n",
       "      <td>0.389569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30973</td>\n",
       "      <td>0.313693</td>\n",
       "      <td>0.440840</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.333085</td>\n",
       "      <td>1.695053</td>\n",
       "      <td>0.675169</td>\n",
       "      <td>5.231218</td>\n",
       "      <td>0.393271</td>\n",
       "      <td>0.603756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>0.372270</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.370577</td>\n",
       "      <td>2.608648</td>\n",
       "      <td>0.413478</td>\n",
       "      <td>0.488592</td>\n",
       "      <td>1.846038e+00</td>\n",
       "      <td>0.634675</td>\n",
       "      <td>0.511846</td>\n",
       "      <td>0.170294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30973</td>\n",
       "      <td>0.313693</td>\n",
       "      <td>0.274339</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.450043</td>\n",
       "      <td>1.695053</td>\n",
       "      <td>0.675169</td>\n",
       "      <td>5.403481</td>\n",
       "      <td>1.960984</td>\n",
       "      <td>0.612210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>0.304911</td>\n",
       "      <td>0.314252</td>\n",
       "      <td>0.370577</td>\n",
       "      <td>2.608648</td>\n",
       "      <td>0.341926</td>\n",
       "      <td>0.268307</td>\n",
       "      <td>1.846038e+00</td>\n",
       "      <td>0.097763</td>\n",
       "      <td>0.511846</td>\n",
       "      <td>0.389569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30973</td>\n",
       "      <td>0.313693</td>\n",
       "      <td>0.325348</td>\n",
       "      <td>0.310702</td>\n",
       "      <td>0.450043</td>\n",
       "      <td>1.695053</td>\n",
       "      <td>0.675169</td>\n",
       "      <td>5.956120</td>\n",
       "      <td>1.323776</td>\n",
       "      <td>0.607324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1564 rows √ó 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5   \\\n",
       "3507  0.383826  0.310702  0.370577  2.608648  0.413478  0.485956   \n",
       "933   0.372270  0.310702  0.003611  2.608648  0.033407  0.488592   \n",
       "192   0.372270  0.298641  0.370577  2.608648  0.260602  0.488592   \n",
       "3421  0.372270  0.310702  0.370577  2.608648  0.413478  0.488592   \n",
       "4414  0.372270  0.310702  0.075568  2.608648  0.413478  0.000087   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "1614  0.372270  0.313919  0.370577  2.608648  0.416547  0.218483   \n",
       "1132  0.214007  0.310702  0.370577  2.608648  0.231212  0.000884   \n",
       "97    0.372270  0.312734  0.340949  2.608648  0.396859  0.488592   \n",
       "2072  0.372270  0.310702  0.370577  2.608648  0.413478  0.488592   \n",
       "3509  0.304911  0.314252  0.370577  2.608648  0.341926  0.268307   \n",
       "\n",
       "                6         7         8         9   ...       47        48  \\\n",
       "3507  1.846038e+00  0.433179  0.511846  0.389569  ...  0.30973  0.317708   \n",
       "933   3.203748e-92  0.634675  0.511846  0.389569  ...  0.30973  0.313693   \n",
       "192   1.846038e+00  0.634675  0.511846  0.252184  ...  0.30973  0.313693   \n",
       "3421  1.846038e+00  0.634675  0.511846  0.389569  ...  0.30973  0.313693   \n",
       "4414  1.846038e+00  0.634675  0.511846  0.173804  ...  0.30973  0.313693   \n",
       "...            ...       ...       ...       ...  ...      ...       ...   \n",
       "1614  1.846038e+00  0.634675  0.511846  0.404618  ...  0.30973  0.313693   \n",
       "1132  1.846038e+00  0.634675  0.511846  0.374232  ...  0.30973  0.313693   \n",
       "97    1.846038e+00  0.634675  0.511846  0.389569  ...  0.30973  0.313693   \n",
       "2072  1.846038e+00  0.634675  0.511846  0.170294  ...  0.30973  0.313693   \n",
       "3509  1.846038e+00  0.097763  0.511846  0.389569  ...  0.30973  0.313693   \n",
       "\n",
       "            49        50        51        52        53        54        55  \\\n",
       "3507  0.424588  0.310702  0.450252  0.245804  0.675169  5.433926  3.208540   \n",
       "933   0.325348  0.310702  0.420278  1.695053  0.675169  4.024478  2.951400   \n",
       "192   0.325348  0.310702  0.197885  1.695053  0.675169  5.990254  1.828165   \n",
       "3421  0.017127  0.000512  0.450043  1.695053  0.675169  6.034251  1.208294   \n",
       "4414  0.325348  0.310702  0.399957  1.695053  0.675169  6.026799  2.227830   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1614  0.391710  0.310702  0.450043  1.643933  0.675169  5.959765  1.697149   \n",
       "1132  0.325348  0.310702  0.292305  1.695053  0.675169  1.448181  0.000009   \n",
       "97    0.440840  0.310702  0.333085  1.695053  0.675169  5.231218  0.393271   \n",
       "2072  0.274339  0.310702  0.450043  1.695053  0.675169  5.403481  1.960984   \n",
       "3509  0.325348  0.310702  0.450043  1.695053  0.675169  5.956120  1.323776   \n",
       "\n",
       "            56  \n",
       "3507  0.355367  \n",
       "933   0.597052  \n",
       "192   0.610941  \n",
       "3421  0.590701  \n",
       "4414  0.595282  \n",
       "...        ...  \n",
       "1614  0.613249  \n",
       "1132  0.612290  \n",
       "97    0.603756  \n",
       "2072  0.612210  \n",
       "3509  0.607324  \n",
       "\n",
       "[1564 rows x 57 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test0=norm1(test,df_zero1,df_zero2)\n",
    "X_test0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spam_probability=X_one*priority1\n",
    "NonSpam_probability=X_zero*priority0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0\n",
       "5     0.0\n",
       "6     0.0\n",
       "7     0.0\n",
       "8     0.0\n",
       "9     0.0\n",
       "10    0.0\n",
       "11    0.0\n",
       "12    0.0\n",
       "13    0.0\n",
       "14    0.0\n",
       "15    0.0\n",
       "16    0.0\n",
       "17    0.0\n",
       "18    0.0\n",
       "19    0.0\n",
       "20    0.0\n",
       "21    0.0\n",
       "22    0.0\n",
       "23    0.0\n",
       "24    0.0\n",
       "25    0.0\n",
       "26    0.0\n",
       "27    0.0\n",
       "28    0.0\n",
       "29    0.0\n",
       "30    0.0\n",
       "31    0.0\n",
       "32    0.0\n",
       "33    0.0\n",
       "34    0.0\n",
       "35    0.0\n",
       "36    0.0\n",
       "37    0.0\n",
       "38    0.0\n",
       "39    0.0\n",
       "40    0.0\n",
       "41    0.0\n",
       "42    0.0\n",
       "43    0.0\n",
       "44    0.0\n",
       "45    0.0\n",
       "46    0.0\n",
       "47    0.0\n",
       "48    0.0\n",
       "49    0.0\n",
       "50    0.0\n",
       "51    0.0\n",
       "52    0.0\n",
       "53    0.0\n",
       "54    0.0\n",
       "55    0.0\n",
       "56    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(Spam_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brindakulkarni/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:51: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_prod(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     NaN\n",
       "4     0.0\n",
       "5     0.0\n",
       "6     0.0\n",
       "7     0.0\n",
       "8     0.0\n",
       "9     0.0\n",
       "10    0.0\n",
       "11    0.0\n",
       "12    0.0\n",
       "13    0.0\n",
       "14    0.0\n",
       "15    0.0\n",
       "16    0.0\n",
       "17    0.0\n",
       "18    0.0\n",
       "19    0.0\n",
       "20    0.0\n",
       "21    0.0\n",
       "22    0.0\n",
       "23    0.0\n",
       "24    0.0\n",
       "25    0.0\n",
       "26    0.0\n",
       "27    0.0\n",
       "28    0.0\n",
       "29    0.0\n",
       "30    0.0\n",
       "31    0.0\n",
       "32    0.0\n",
       "33    0.0\n",
       "34    0.0\n",
       "35    0.0\n",
       "36    0.0\n",
       "37    0.0\n",
       "38    0.0\n",
       "39    0.0\n",
       "40    0.0\n",
       "41    0.0\n",
       "42    0.0\n",
       "43    0.0\n",
       "44    0.0\n",
       "45    0.0\n",
       "46    0.0\n",
       "47    0.0\n",
       "48    0.0\n",
       "49    0.0\n",
       "50    0.0\n",
       "51    0.0\n",
       "52    0.0\n",
       "53    0.0\n",
       "54    0.0\n",
       "55    0.0\n",
       "56    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(NonSpam_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1564, 1)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 0, 'FP': 0, 'TN': 34, 'FN': 23}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = {'TP':0, 'FP':0, 'TN': 0, 'FN':0}\n",
    "count=0\n",
    "for sp,nsp, gt in zip(X_test1,X_test0, Y_test):\n",
    "    count=count+1\n",
    "    sp_mul = np.prod(sp)\n",
    "    nsp_mul = np.prod(nsp)\n",
    "    #print(nsp_mul)\n",
    "    pred = 1 if sp_mul > nsp_mul else 0\n",
    "    if pred == 0 and gt == 0:\n",
    "        matrix['TN'] +=1\n",
    "    elif pred == 1 and gt == 1:\n",
    "        matrix['TP'] +=1\n",
    "    elif pred == 0 and gt == 1:\n",
    "        matrix['FN'] +=1\n",
    "    elif pred == 1 and gt == 0:\n",
    "        matrix['FP'] +=1\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-3e01da6eddda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# b. Recall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Recall: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "precision = matrix['TP']/(matrix['TP']+matrix['FP'])\n",
    "print('Precision: ',precision)\n",
    "# b. Recall\n",
    "recall = matrix['TP']/(matrix['TP']+matrix['FN'])\n",
    "print('Recall: ',recall)\n",
    "# c. F-measure\n",
    "F1 = (2*precision*recall)/(precision+recall)\n",
    "print('F1: ',F1)\n",
    "# d. Accuracy\n",
    "acc = (matrix['TP']+matrix['TN'])/(matrix['TP']+matrix['TN']+matrix['FP']+matrix['FN'])\n",
    "print(\"Accuracy: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6227621483375959"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "# fit the model\n",
    "model.fit(train, Y_train)\n",
    "model.score(test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5  Decision Trees\n",
    "\n",
    "Let‚Äôs train and test a Decision Tree to classify Spam or Not from the Spambase Dataset.\n",
    "\n",
    "__Write a script that:__\n",
    "\n",
    "1. Reads in the data.\n",
    "2. Randomizes the data.\n",
    "3. Selects the first 2/3 (round up) of the data for training and the remaining for testing\n",
    "4. Standardizes the data (except for the last column of course) using the training data\n",
    "5. Divides the training data into two groups:  Spam samples, Non-Spam samples.\n",
    "6. Trains a decision tree using the ID3 algorithm without any pruning.\n",
    "7. Classify each testing sample using your trained decision tree.\n",
    "8. Computes the following statistics using the testing data results:<br>\n",
    " a.  Precision <br>\n",
    " b.  Recall<br>\n",
    " c.  F-measure<br>\n",
    " d.  Accuracy<br>\n",
    " \n",
    "__Implementation Details__\n",
    "\n",
    "1.  Seed the random number generate with zero prior to randomizing the data\n",
    "2.  Depending on your perspective, the features are either continuous or finite discretize.  The lattercan be considered tru since the real-values are just the number of times a feature is observedin an email, normalized by some other count.  That being said, for a decision tree we normallyuse categorical or discretized features. So for the purpose of this dataset, look at therange of each feature and turn them into binary features by choosing a threshold. I suggest using the median or mean.\n",
    "\n",
    "__In your report you will need:__ \n",
    "1.  The statistics requested for your Decision Tree classifier run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Notes hand typed from Wikipedia; nearly verbatim_\n",
    "\n",
    "__Algorithm__\n",
    "\n",
    "The ID3 - iterative dichotomiser 3 - is used to generate a decision tree from a dataset. ID3 is the precusor to C4.5 and is typically used in machine learning and natural language processing domains.\n",
    "\n",
    "The ID3 algorithm begins witht the origianl set S as the root node. On each iteration of the algorithm, it iterates through every unused attribute of the set S and calculates the entropy H(Y) or the information gain IG(S) of that attribute. \n",
    "\n",
    "It then selects the attribute which has the smallest entropy (or largest information gain) value. The set S is then split or partitioned be the selected attribute to produce subsets of the data. For example, a node can be split into child nodes based on the subsets of the population whose ages are less than 50, between 50 and 100, and greater than 100). The algorithm continues to recurse on each subset, considering only attributes never slected before.\n",
    "\n",
    "Recursion on a subset may stop in one of these cases:\n",
    "\n",
    "- every element in the subset belongs to the same class; in which case the node is turned into a leaf node and labelled with the class of the examples.\n",
    "- there are no more attributes to be selected, but the examples still do not belong to the same class. In this case, the node is made a leaf node and lablled with the most common class of the examples in the subset.\n",
    "- ther are no examples in the subset, which happens when no example in the parent set was found to match a specific value of the selected attribute. An example could be the absence of a person among the population with age over 100 years. The leaf node is created and labelled with the most common class of the examples in the parent nodes set.\n",
    "\n",
    "Throughout the algorithm, the decision tree is constructed with each non-terminal node (internal node) representing the selected attribute on which the data was split, and terminal nodes (leaf nodes) representing the class label of the final subset of this branch.\n",
    "\n",
    "\n",
    "__Summary__\n",
    "1. Calculate the entropy of every attribute $\\alpha$ of $S$.\n",
    "2. Partition (split) the set $S$ into subsets using the attribute for which the resulting entropy after splitting is minimized; or equivalenty, information gain is maximum.\n",
    "3. Make a decision tree node containing that attribute.\n",
    "4. Recurse on subsets using the remaining attributes.\n",
    "\n",
    "\n",
    "__Pseudocode__\n",
    "\n",
    "```\n",
    "ID3 (Examples, Target_Attribute, Attributes)\n",
    "    \n",
    "    Create a root node for the tree\n",
    "    \n",
    "    If all examples are positive: Return the single-node tree Root with label = +\n",
    "    If all examples are negative: Return the single-node tree Root with label = -\n",
    "    If number of predicting attributes is empty: Return the single-node tree Root\n",
    "    \n",
    "    If Root not None: label = most common value of the target attribute in the example\n",
    "    \n",
    "    Else If Root is None:\n",
    "            \n",
    "            A = the attribute the best classifies the examples\n",
    "            Decision Tree attribute for Root = A\n",
    "            \n",
    "            For value $v_i$ in A:\n",
    "                Add a new tree branch below Root, correspongin to the test A = $v_i$\n",
    "                Let Examples($v_i$) be the subset of examples that have the value $v_i$ for A\n",
    "                If Examples($v_i$) is empty:\n",
    "                    Then below this new branch add a leaf node with label = most common target value in the examples\n",
    "                Else below this new branch add the subtree ID3(Examples($v_i$), Target_attribute, Attributes - {A})\n",
    "    \n",
    "    End\n",
    "    Return Root\n",
    "```\n",
    "\n",
    "__Properties__\n",
    "ID3 does not guarantee an optimal solution. It can converge on local optima. It uses a greedy strategy by selecting the locally best attribute to split the dataset on each iteration. The algorithms optimality can be improved by using backtracking during the search for the optimal decision tree at the cost of possibly taking longer.\n",
    "\n",
    "ID3 can overfit the training data. To avoid overfitting, smaller decision trees should be preferred over larger ones. This algorithm usually produces small trees but it does not always produce the smallest tree possible.\n",
    "\n",
    "ID3 is harder to use on continuous data than on factored data (factored data has a discrete number of possible values, thus reducing the possible barnch points). If the values of any given attribute are continuous, then there are many more places to split the data on this attribute, and searching for the best value to split on can be time consuming. \n",
    "\n",
    "__Usage__\n",
    "ID3 is used by training on data set $S$ to produce a decision tree which is stored in memory. At runtime, this decision tree is used to classify new test cases (feature vectors) by traversing the decision tree using the features of the datum to arrive at a leaf node. The class of this terminal node is the class the test case is classified as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1=pd.read_csv(\"spambase.data\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training set: 3037\n",
      "No. of testing set: 1564\n"
     ]
    }
   ],
   "source": [
    "train = df1.sample(frac=0.66, random_state=42)\n",
    "test = df1.drop(train.index)\n",
    "print(f\"No. of training set: {train.shape[0]}\")\n",
    "print(f\"No. of testing set: {test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardise(string1):\n",
    "    m1=np.mean(string1)\n",
    "    std1=np.std(string1)\n",
    "    string2=(string1-m1)/std1\n",
    "    return string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=train[57]\n",
    "train=train.drop([57],axis=1)\n",
    "train=train.apply(Standardise)\n",
    "train[57]=Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-173-3fda9f158b7a>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-173-3fda9f158b7a>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    string2=0\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def Change_value(string1):\n",
    "    m1=np.mean(string1)\n",
    "    i=0\n",
    "    while i<string1.shape[0]:\n",
    "        if string[i1<m1:\n",
    "           string2=0\n",
    "        else:\n",
    "            string2=1\n",
    "        i=i+1\n",
    "    return string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-8249aaf952f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChange_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7766\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7767\u001b[0m         )\n\u001b[0;32m-> 7768\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7770\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-835f1a61a045>\u001b[0m in \u001b[0;36mChange_value\u001b[0;34m(string1)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mChange_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mm1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mstring1\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mstring2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1443\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "train=train.apply(Change_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zero = df1[df1[57] == 0]\n",
    "index1 = df_zero.index\n",
    "index1.name = \"Non Spam Samples\"\n",
    "df_one = df1[df1[57]==1]\n",
    "index = df_one.index\n",
    "index.name = \"Non Spam Samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non Spam Samples</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>3.482</td>\n",
       "      <td>5</td>\n",
       "      <td>5902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.040</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.625</td>\n",
       "      <td>7</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>4.411</td>\n",
       "      <td>28</td>\n",
       "      <td>1866</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2788 rows √ó 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0    1     2    3     4     5    6    7    8     9   ...  \\\n",
       "Non Spam Samples                                                         ...   \n",
       "1813              0.00  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.0  0.00  ...   \n",
       "1814              0.00  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.0  0.85  ...   \n",
       "1815              0.00  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.0  0.00  ...   \n",
       "1816              0.00  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.0  0.00  ...   \n",
       "1817              0.00  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.0  0.00  ...   \n",
       "...                ...  ...   ...  ...   ...   ...  ...  ...  ...   ...  ...   \n",
       "4596              0.31  0.0  0.62  0.0  0.00  0.31  0.0  0.0  0.0  0.00  ...   \n",
       "4597              0.00  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.0  0.00  ...   \n",
       "4598              0.30  0.0  0.30  0.0  0.00  0.00  0.0  0.0  0.0  0.00  ...   \n",
       "4599              0.96  0.0  0.00  0.0  0.32  0.00  0.0  0.0  0.0  0.00  ...   \n",
       "4600              0.00  0.0  0.65  0.0  0.00  0.00  0.0  0.0  0.0  0.00  ...   \n",
       "\n",
       "                     48     49     50     51     52     53     54  55    56  \\\n",
       "Non Spam Samples                                                              \n",
       "1813              0.022  0.022  0.019  0.022  0.022  0.022  3.482   5  5902   \n",
       "1814              0.299  0.000  0.000  0.149  0.000  0.000  1.040   2    26   \n",
       "1815              0.000  0.000  0.000  0.000  0.000  0.000  1.000   1     3   \n",
       "1816              0.000  0.131  0.000  0.262  0.000  0.000  1.625   7    65   \n",
       "1817              0.000  0.104  0.324  0.000  0.000  0.011  4.411  28  1866   \n",
       "...                 ...    ...    ...    ...    ...    ...    ...  ..   ...   \n",
       "4596              0.000  0.232  0.000  0.000  0.000  0.000  1.142   3    88   \n",
       "4597              0.000  0.000  0.000  0.353  0.000  0.000  1.555   4    14   \n",
       "4598              0.102  0.718  0.000  0.000  0.000  0.000  1.404   6   118   \n",
       "4599              0.000  0.057  0.000  0.000  0.000  0.000  1.147   5    78   \n",
       "4600              0.000  0.000  0.000  0.125  0.000  0.000  1.250   5    40   \n",
       "\n",
       "                  57  \n",
       "Non Spam Samples      \n",
       "1813               0  \n",
       "1814               0  \n",
       "1815               0  \n",
       "1816               0  \n",
       "1817               0  \n",
       "...               ..  \n",
       "4596               0  \n",
       "4597               0  \n",
       "4598               0  \n",
       "4599               0  \n",
       "4600               0  \n",
       "\n",
       "[2788 rows x 58 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non Spam Samples</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>2.600</td>\n",
       "      <td>42</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.740</td>\n",
       "      <td>13</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.685</td>\n",
       "      <td>62</td>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.391</td>\n",
       "      <td>66</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.049</td>\n",
       "      <td>3.446</td>\n",
       "      <td>318</td>\n",
       "      <td>1003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1813 rows √ó 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0     1     2    3     4     5     6     7     8     9   \\\n",
       "Non Spam Samples                                                              \n",
       "0                 0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00   \n",
       "1                 0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94   \n",
       "2                 0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25   \n",
       "3                 0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63   \n",
       "4                 0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63   \n",
       "...                ...   ...   ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "1808              0.00  0.00  0.00  0.0  0.00  0.23  0.00  0.00  0.00  0.00   \n",
       "1809              0.39  0.00  0.00  0.0  0.00  0.39  0.79  0.00  0.00  0.39   \n",
       "1810              0.00  0.00  0.77  0.0  0.38  0.38  0.38  0.00  0.00  0.77   \n",
       "1811              0.00  0.00  0.00  0.0  0.53  0.00  0.53  0.00  0.53  0.00   \n",
       "1812              0.00  0.31  0.42  0.0  0.00  0.10  0.00  0.52  0.21  0.52   \n",
       "\n",
       "                  ...     48     49     50     51     52     53     54   55  \\\n",
       "Non Spam Samples  ...                                                         \n",
       "0                 ...  0.000  0.000  0.000  0.778  0.000  0.000  3.756   61   \n",
       "1                 ...  0.000  0.132  0.000  0.372  0.180  0.048  5.114  101   \n",
       "2                 ...  0.010  0.143  0.000  0.276  0.184  0.010  9.821  485   \n",
       "3                 ...  0.000  0.137  0.000  0.137  0.000  0.000  3.537   40   \n",
       "4                 ...  0.000  0.135  0.000  0.135  0.000  0.000  3.537   40   \n",
       "...               ...    ...    ...    ...    ...    ...    ...    ...  ...   \n",
       "1808              ...  0.077  0.038  0.000  0.000  0.000  0.038  2.600   42   \n",
       "1809              ...  0.000  0.064  0.000  0.640  0.192  0.000  2.740   13   \n",
       "1810              ...  0.063  0.127  0.255  0.510  0.000  0.000  3.685   62   \n",
       "1811              ...  0.000  0.000  0.000  0.082  0.000  0.000  4.391   66   \n",
       "1812              ...  0.000  0.016  0.000  0.887  0.032  0.049  3.446  318   \n",
       "\n",
       "                    56  57  \n",
       "Non Spam Samples            \n",
       "0                  278   1  \n",
       "1                 1028   1  \n",
       "2                 2259   1  \n",
       "3                  191   1  \n",
       "4                  191   1  \n",
       "...                ...  ..  \n",
       "1808               182   1  \n",
       "1809                74   1  \n",
       "1810               258   1  \n",
       "1811               101   1  \n",
       "1812              1003   1  \n",
       "\n",
       "[1813 rows x 58 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
